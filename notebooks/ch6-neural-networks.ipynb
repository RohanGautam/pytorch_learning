{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b3e614b3786c960a93b9d5d36e7aaa38532a0ca506246ed178fb5ba00ff82899",
   "display_name": "Python 3.7.10 64-bit ('ml_learning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Neural networks\n",
    "### mental model of learning\n",
    "![](../images/model_of_learning.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nipykernel_launcher:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([9, 8, 0, 3, 2, 5, 1, 4, 7]), tensor([10,  6]))"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# same example data as chapter 5\n",
    "t_c = torch.tensor([0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]) # in celcius\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]) # in unknown unit\n",
    "# This makes the size (11) to (11,1)\n",
    "# basically we added an extra dimension of \"1\" at the end\n",
    "# Pytorch expects batch size at the 0th position\n",
    "# here the batch size is 11 (11 data points) and there is just one feature(the temp). That's why we do that.\n",
    "# WE want to reshape it in that form before passing it to the network, as it expects it in that form.\n",
    "t_c = torch.tensor(t_c).unsqueeze(1)  \n",
    "t_u = torch.tensor(t_u).unsqueeze(1)\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "t_un_val.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-5.6589],\n",
       "        [-2.5575]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "# has a __call__ method defined already, which sets up \"hooks\",ie, things for the forward call to work correctly\n",
    "linear_model = nn.Linear(1,1) #(input and output tensor size)\n",
    "linear_model(t_un_val) # alternative to calling forward with the same arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8990]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4899], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090],\n",
       "        [-0.4090]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# can have results for a batch, where the zeroth dimension withh be the batch size\n",
    "linear_model(torch.ones(10,1))"
   ]
  },
  {
   "source": [
    "## using the linear model\n",
    "- it's in the nn module, but, not a neural network yet.\n",
    "- Even the results we get are the exact same as before."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, optimizer, loss_fn, t_u_train, t_c_train, t_u_val, t_c_val):\n",
    "    for i in range(1, n_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_predicted = model(t_u_train)\n",
    "        train_loss = loss_fn(train_predicted, t_c_train)\n",
    "        val_loss = loss_fn(model(t_u_val), t_c_val)\n",
    "        train_loss.backward() # calc gradients\n",
    "        optimizer.step() # update parameters\n",
    "\n",
    "        if i%500==0:\n",
    "            print(f\"epoch {i} training loss {train_loss}, validation loss {val_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 500 training loss 8.659355163574219, validation loss 7.074272155761719\n",
      "epoch 1000 training loss 3.8009579181671143, validation loss 4.253093242645264\n",
      "epoch 1500 training loss 2.8559889793395996, validation loss 4.459417343139648\n",
      "epoch 2000 training loss 2.6721911430358887, validation loss 4.8325395584106445\n",
      "epoch 2500 training loss 2.636443614959717, validation loss 5.051978588104248\n",
      "epoch 3000 training loss 2.629490613937378, validation loss 5.159413814544678\n",
      "epoch 3500 training loss 2.6281378269195557, validation loss 5.208893775939941\n",
      "epoch 4000 training loss 2.627875328063965, validation loss 5.231110572814941\n",
      "epoch 4500 training loss 2.627823829650879, validation loss 5.240985870361328\n",
      "epoch 5000 training loss 2.6278128623962402, validation loss 5.2453765869140625\n",
      "[Parameter containing:\n",
      "tensor([[5.4199]], requires_grad=True), Parameter containing:\n",
      "tensor([-17.9863], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "## the optimizer with optimize the params of the linear model\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    model = linear_model,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = nn.MSELoss(), # MSELoss is a class, we instantiate it and pass the function to the loop\n",
    "    t_u_train = t_un_train,\n",
    "    t_c_train = t_c_train,\n",
    "    t_u_val = t_un_val,\n",
    "    t_c_val = t_c_val,\n",
    "\n",
    ")\n",
    "print(list(linear_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}