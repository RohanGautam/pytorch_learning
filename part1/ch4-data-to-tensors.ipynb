{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd047d517df254fbe59b5c04b69f23871c524fea7fc7dc51e349dcd158d9476f98d",
   "display_name": "Python 3.7.10 64-bit ('pytorch_latest_p37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch"
   ]
  },
  {
   "source": [
    "## for one image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'imageio.core.util.Array'>\n(1003, 1999, 3)\n"
     ]
    }
   ],
   "source": [
    "img_arr = imageio.imread(\"../data/dog.jpg\")\n",
    "print(type(img_arr)) # numpy-like array object\n",
    "print(img_arr.shape) # height, width, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1003, 1999, 3])\ntorch.Size([3, 1003, 1999])\n"
     ]
    }
   ],
   "source": [
    "# torch expects it in the form : channel, height, width\n",
    "img = torch.from_numpy(img_arr)\n",
    "print(img.shape)\n",
    "out = img.permute(2,0,1)\n",
    "print(out.shape)"
   ]
  },
  {
   "source": [
    "## for multiple images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  it's more efficient to first build up a tensor structure then populate it\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for i, name in enumerate(os.listdir(\"../data/p1ch4/image-cats\")):\n",
    "    img_arr = imageio.imread(f\"../data/p1ch4/image-cats/{name}\")\n",
    "    img = torch.from_numpy(img_arr)\n",
    "    out = img.permute(2,0,1)[:3] # discard alpha channel if present\n",
    "    batch[i] = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing - bw [0,1] or [-1,1]\n",
    "# this is beacuse they train better when normalized\n",
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "# or, calculate mean and sd and scale so that op has zero mean and unit sd across each channel"
   ]
  },
  {
   "source": [
    "## loading a ct scan"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%94/99 files (94.9%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 42/99  (42.487/99  (87.999/99  (100.0%)\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"../data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([99, 512, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr).float()\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "vol = torch.unsqueeze(vol, 0)\n",
    "vol.shape\n"
   ]
  },
  {
   "source": [
    "## timeseries data\n",
    "has a dimension of time, whic his useful if we want to exploit causal relationships across a time period"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_numpy = np.loadtxt(\"../data/p1ch4/bike-sharing-dataset/hour-fixed.csv\",\n",
    "    dtype=np.float32,\n",
    "    delimiter=\",\",\n",
    "    skiprows=1,\n",
    "    converters={1: lambda x: float(x[8:10])}\n",
    ")\n",
    "bikes = torch.from_numpy(bikes_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "bikes.shape, bikes.stride()\n",
    "# 17520 hours, 17 axes/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([730, 24, 17])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "daily_bikes.shape"
   ]
  }
 ]
}